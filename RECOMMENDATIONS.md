# BigQuery Optimization Recommendations

This document provides detailed explanations of the optimization recommendations generated by the BigQuery Optimization Analysis Tool. Each recommendation includes an explanation, potential cost savings, implementation guidance, and considerations.

## Table of Contents

### Cost Optimization
- [Partition Large Tables](#partition-large-tables)
- [Replace Sharded Tables](#replace-sharded-tables)
- [Require Partition Filters](#require-partition-filters)
- [Prune Large Tables](#prune-large-tables)
- [Set Table Expiration](#set-table-expiration)
- [Expire Old Tables](#expire-old-tables)
- [Use Batch Loads](#use-batch-loads)

### Performance Optimization
- [Cluster Partitioned Tables](#cluster-partitioned-tables)
- [Use Materialized Views](#use-materialized-views)
- [Materialize Views](#materialize-views)
- [Use BI Engine](#use-bi-engine)
- [Flatten Nested Schema](#flatten-nested-schema)
- [Use Incremental Loads](#use-incremental-loads)

### Governance & Management
- [Add Labels](#add-labels)
- [Add Descriptions](#add-descriptions)
- [Use Descriptive Names](#use-descriptive-names)

## Cost Optimization

### Partition Large Tables
**Recommendation:** Consider partitioning large tables (>1GB) to improve query performance and reduce costs.

**Potential Savings:** 20-50% cost reduction

**Explanation:**  
Partitioning divides your table into segments based on a TIMESTAMP/DATE column (such as date_created or transaction_date) or an INTEGER column. When you query with filters on the partitioning column, BigQuery only scans the relevant partitions, significantly reducing the amount of data processed and therefore the query cost.

**Implementation:**  
```sql
CREATE TABLE dataset.new_partitioned_table
PARTITION BY DATE(timestamp_column)
AS SELECT * FROM dataset.original_large_table;
```

**Considerations:**
- Choose a partitioning column that's frequently used in query filters
- For time-based partitioning, consider the granularity (daily, monthly, yearly)
- For tables approaching partition limits (4000 partitions), consider using a coarser granularity
- Consider clustering within partitions for additional performance

### Replace Sharded Tables
**Recommendation:** Replace date-sharded tables with partitioned tables for better performance and manageability.

**Potential Savings:** Up to 80% scan cost reduction

**Explanation:**  
Sharded tables (named like `table_20220101`, `table_20220102`, etc.) were a legacy approach before partitioning was available. They require complex query patterns like `TABLE_WILDCARD` or `UNION ALL` queries, which often scan more data than necessary. Partitioned tables simplify queries and automatically limit scans to relevant partitions.

**Implementation:**  
1. Create a partitioned table:
```sql
CREATE TABLE dataset.unified_partitioned_table
PARTITION BY DATE(date_column)
AS SELECT * FROM dataset.table_*
```
2. Update queries to use the new table with partition filters

**Considerations:**
- Ensure all queries are updated to use the new partitioned table
- Set appropriate access controls on the new table
- Consider adding clustering if the original queries filtered on additional columns

### Require Partition Filters
**Recommendation:** Enforce partition filters on partitioned tables to prevent accidental full table scans.

**Potential Savings:** 30-90% cost reduction

**Explanation:**  
When a partitioned table is queried without a partition filter, BigQuery scans the entire table, eliminating the cost benefits of partitioning. Requiring partition filters prevents accidental full table scans that could incur unexpected costs.

**Implementation:**  
```sql
ALTER TABLE dataset.partitioned_table
SET OPTIONS (
  require_partition_filter = true
);
```

**Considerations:**
- Test existing queries to ensure they include partition filters
- Update any dashboards or scheduled queries that might be affected
- Provide documentation to team members on required filters
- Implement proper error handling in applications that query this table

### Prune Large Tables
**Recommendation:** For very large tables (>10GB), consider pruning unused columns or implementing more selective filters.

**Potential Savings:** 10-30% cost saving

**Explanation:**  
Large tables with many columns often include data that's rarely or never queried. By removing unused columns or implementing more selective filters, you can reduce the amount of data scanned and therefore lower query costs.

**Implementation:**  
1. Analyze query patterns to identify unused or rarely used columns
2. Create a view or new table with only the necessary columns:
```sql
CREATE TABLE dataset.optimized_table
AS SELECT col1, col2, col3 -- only frequently used columns
FROM dataset.original_large_table;
```

**Considerations:**
- Monitor query patterns over time to identify truly unused columns
- Consider creating views for specific access patterns
- Implement column-level access controls as needed
- Test performance impact before making permanent changes

### Set Table Expiration
**Recommendation:** Set expiration policies for large tables (>2GB) to automatically manage storage costs.

**Potential Savings:** 10-30% long-term storage savings

**Explanation:**  
Data that's no longer needed continues to incur storage costs. By setting table expiration, you can automatically remove data when it's no longer needed, reducing long-term storage costs and maintaining a clean data environment.

**Implementation:**  
```sql
ALTER TABLE dataset.table
SET OPTIONS (
  expiration_timestamp = TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)
);
```

**Considerations:**
- Consider your data retention requirements and compliance needs
- Implement a backup strategy for data that might be needed later
- Set different expiration times for different types of data
- Use table decorators (@-TIME) to access historical data if needed

### Expire Old Tables
**Recommendation:** Apply expiration or consider archiving for tables not modified in >90 days.

**Potential Savings:** 20-40% storage saving

**Explanation:**  
Tables that haven't been modified in a long time are often no longer actively used but continue to incur storage costs. Setting expiration policies or archiving to lower-cost storage can significantly reduce storage costs.

**Implementation:**  
1. For expiration:
```sql
ALTER TABLE dataset.old_table
SET OPTIONS (
  expiration_timestamp = TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
);
```
2. For archiving to long-term storage:
```sql
EXPORT DATA OPTIONS(
  uri='gs://bucket/archive/old_table_*.avro',
  format='AVRO') AS
SELECT * FROM dataset.old_table;
```

**Considerations:**
- Verify that the table is truly not needed before archiving or setting expiration
- Consider creating a catalog of archived data for reference
- For regulatory data, confirm compliance requirements before expiring

### Use Batch Loads
**Recommendation:** Replace streaming inserts with batch loads where possible.

**Potential Savings:** 50%+ cheaper than streaming

**Explanation:**  
Streaming inserts into BigQuery are convenient but more expensive than batch loads. They also result in streaming buffers that can't be fully optimized for query performance. For data that doesn't require real-time insertion, batch loading is significantly more cost-effective.

**Implementation:**  
1. Collect data into files (e.g., Cloud Storage)
2. Schedule regular batch loads:
```sql
LOAD DATA INTO dataset.table
FROM FILES(
  format='CSV',
  uris=['gs://bucket/path/to/file.csv']
);
```

**Considerations:**
- Evaluate if your use case truly requires real-time data
- Consider micro-batching (e.g., every 5-15 minutes) as a compromise
- Use appropriate file formats (Avro, Parquet, or ORC) for optimal performance
- Implement proper error handling for batch load failures

## Performance Optimization

### Cluster Partitioned Tables
**Recommendation:** Add clustering to large partitioned tables (>5GB) for additional query performance.

**Potential Savings:** 20-40% query performance improvement

**Explanation:**  
Clustering organizes data within each partition based on the contents of specific columns, which can further reduce the amount of data scanned in queries that filter or aggregate on those columns. This is especially effective for large partitioned tables.

**Implementation:**  
```sql
CREATE TABLE dataset.partitioned_clustered_table
PARTITION BY DATE(date_column)
CLUSTER BY category, region
AS SELECT * FROM dataset.original_table;
```

**Considerations:**
- Choose clustering columns that are frequently used in query filters or joins
- Order clustering columns from highest to lowest cardinality
- Limit to 1-4 clustering columns for best performance
- Remember that clustering is automatically maintained by BigQuery

### Use Materialized Views
**Recommendation:** Consider materialized views for frequent queries on large tables (>5GB).

**Potential Savings:** 30-70% cost/performance improvement

**Explanation:**  
Materialized views store pre-computed results of queries, significantly improving performance for repetitive queries. BigQuery automatically maintains these views, keeping them updated as the underlying data changes, and intelligently uses them when applicable queries are run.

**Implementation:**  
```sql
CREATE MATERIALIZED VIEW dataset.mv_name
AS SELECT
  date,
  product,
  SUM(revenue) as total_revenue
FROM dataset.large_table
GROUP BY 1, 2;
```

**Considerations:**
- Identify common query patterns that would benefit from materialization
- Materialized views incur storage costs but save on query costs
- BigQuery has specific restrictions on materialized views (e.g., no window functions)
- Monitor usage to ensure views are being utilized

### Materialize Views
**Recommendation:** Convert regular views to materialized views for frequently accessed views.

**Potential Savings:** 30-60% repeated query savings

**Explanation:**  
Regular views compute their results each time they're queried, which can be expensive for complex views accessed frequently. Converting them to materialized views pre-computes and stores results, significantly improving performance and reducing costs for repetitive access patterns.

**Implementation:**  
1. Identify frequently accessed views
2. Create a materialized version:
```sql
CREATE MATERIALIZED VIEW dataset.materialized_view_name
AS SELECT * FROM dataset.original_view_definition;
```

**Considerations:**
- Consider storage costs versus query savings
- Ensure the materialized view definition meets BigQuery's requirements
- Review query patterns to confirm the view will be utilized
- Consider incremental refresh strategies for the materialized data

### Use BI Engine
**Recommendation:** Consider BI Engine for dashboards querying tables larger than 1GB.

**Potential Savings:** Up to 50% performance gain

**Explanation:**  
BigQuery BI Engine is an in-memory analysis service that accelerates SQL queries by creating a columnar in-memory cache of your data. It's especially effective for interactive dashboards and reports that repeatedly query the same tables.

**Implementation:**  
1. Configure BigQuery BI Engine in the Google Cloud Console
2. Allocate memory to specific projects or tables
3. No query changes needed - BI Engine acceleration is applied automatically

**Considerations:**
- BI Engine has additional costs but often pays for itself in query savings
- Start with a small allocation and scale based on performance metrics
- Most effective for dashboards with multiple users and frequent refreshes
- Works best with Looker, Data Studio, and other BI tools connected to BigQuery

### Flatten Nested Schema
**Recommendation:** Consider flattening nested schemas in large tables (>1GB) for better query performance.

**Potential Savings:** 10-25% query cost reduction

**Explanation:**  
While nested and repeated fields can model complex data structures, they can also make queries more complex and less efficient. Flattening these structures into a denormalized form can simplify queries and improve performance for large tables.

**Implementation:**  
```sql
CREATE TABLE dataset.flattened_table
AS SELECT
  id,
  nested_field.property1 as property1,
  nested_field.property2 as property2
FROM dataset.table_with_nested_schema;
```

**Considerations:**
- Consider maintaining both nested and flattened versions for different use cases
- Understand the trade-offs between storage efficiency and query performance
- Create views that present flattened data in a structured format
- Test query performance before and after flattening

### Use Incremental Loads
**Recommendation:** Implement incremental loads for large tables (>5GB) that are frequently updated.

**Potential Savings:** 20-40% cost saving

**Explanation:**  
Complete table refreshes for large tables are expensive in terms of processing and storage costs. Implementing incremental loads that only process new or changed data can significantly reduce costs and processing time.

**Implementation:**  
1. Track a last-modified timestamp or incremental ID in your source data
2. Use merge statements to apply only the changes:
```sql
MERGE dataset.target_table T
USING dataset.new_data S
ON T.id = S.id
WHEN MATCHED THEN
  UPDATE SET col1 = S.col1, col2 = S.col2
WHEN NOT MATCHED THEN
  INSERT (id, col1, col2) VALUES (S.id, S.col1, S.col2);
```

**Considerations:**
- Implement reliable change tracking in source systems
- Consider partition strategies that align with your incremental load pattern
- Implement error handling and validation for incremental processes
- Set up monitoring to detect and resolve data drift

## Governance & Management

### Add Labels
**Recommendation:** Add labels to tables for better governance and cost attribution.

**Potential Savings:** Indirect cost control benefits

**Explanation:**  
Table labels provide metadata that can be used for governance, cost attribution, and resource management. They make it easier to track usage patterns, allocate costs to the right departments or projects, and enforce policies based on resource classification.

**Implementation:**  
```sql
ALTER TABLE dataset.table
SET OPTIONS (
  labels = [('environment', 'production'), ('department', 'finance'), ('application', 'reporting')]
);
```

**Considerations:**
- Develop a consistent labeling strategy across your organization
- Include labels for cost center, environment, application, and data sensitivity
- Automate label application where possible
- Use labels in combination with IAM policies for fine-grained access control

### Add Descriptions
**Recommendation:** Add descriptions to tables that lack them to improve discoverability.

**Potential Savings:** Reduces accidental queries by approximately 10%

**Explanation:**  
Clear table and column descriptions improve data discoverability and understanding, reducing the likelihood of incorrect queries or misinterpretation of data. This leads to fewer wasted queries and better decision-making based on the data.

**Implementation:**  
```sql
ALTER TABLE dataset.table
SET OPTIONS (
  description = "This table contains daily sales transactions with customer and product information."
);

-- For column descriptions
ALTER TABLE dataset.table
ALTER COLUMN column_name
SET OPTIONS (description = "Unique transaction identifier");
```

**Considerations:**
- Include information about data source, refresh frequency, and intended use
- Document business definitions of metrics and dimensions
- Update descriptions when table structure or content purpose changes
- Consider implementing a data catalog solution for more comprehensive metadata

### Use Descriptive Names
**Recommendation:** Use clear, descriptive names for tables instead of numeric or cryptic identifiers.

**Potential Savings:** Indirect cost avoidance

**Explanation:**  
Descriptive table names make it easier for users to find and understand data, reducing the need for exploratory queries and helping prevent duplicate data creation. This leads to more efficient use of resources and better data governance.

**Implementation:**  
```sql
CREATE TABLE dataset.sales_transactions_daily
AS SELECT * FROM dataset.tbl_01234;
```

**Considerations:**
- Develop naming conventions that reflect data content and purpose
- Include relevant time granularity in names of time-series data
- Avoid abbreviations that might be unclear to new team members
- Consider adding environment or status indicators (e.g., _prod, _staging)